#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸæ­£ Google embedding çš„ MERLIN å‚æ•°è°ƒä¼˜ç³»ç»Ÿ
ç”¨äºæ‰¾åˆ°æœ€ä¼˜çš„ (m, Î±, Î²) å‚æ•°ç»„åˆ

å®éªŒè®¾è®¡ï¼š
é˜¶æ®µä¸€ï¼šç†µå€¼åˆ†å¸ƒæµ‹é‡ - ä½¿ç”¨10-20ä¸ªæ ·æœ¬æµ‹é‡çœŸå®çš„ç†µå€¼åˆ†å¸ƒ
é˜¶æ®µäºŒï¼šåŸºäºåˆ†å¸ƒç»“æœç¡®å®šå‚æ•°æœç´¢ç©ºé—´
é˜¶æ®µä¸‰ï¼šç½‘æ ¼æœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆ
"""

import os
import sys
import json
import time
import random
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Tuple
import itertools
import asyncio
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import argparse

# è‡ªåŠ¨è®¾ç½®PYTHONPATH
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.logger import logger, setup_logger
from utils.env_utils import load_env_variables, get_required_env
from merlin.questioner import Questioner
from merlin.reranker import Reranker

class EntropyDistributionAnalyzer:
    """
    ç†µå€¼åˆ†å¸ƒåˆ†æå™¨ - ç”¨äºæµ‹é‡ç³»ç»Ÿå†…åœ¨çš„ç†µå€¼åˆ†å¸ƒæƒ…å†µ
    """
    
    def __init__(self, 
                 data_path: str = "/home/peterchen/M2/ADEPT/data/mafw",
                 excel_path: str = "/home/peterchen/M2/ADEPT/data/mafw/labels/sampled_850.xlsx",
                 video_dir: str = "/home/peterchen/M2/ADEPT/data/mafw/videos",
                 output_dir: str = "/home/peterchen/M2/ADEPT/data/mafw/entropy_analysis_outputs",
                 env_file: str = "/home/peterchen/M2/ADEPT/.env",
                 sample_size: int = 400):
        """
        åˆå§‹åŒ–ç†µå€¼åˆ†å¸ƒåˆ†æå™¨
        
        Args:
            data_path: æ•°æ®è·¯å¾„
            excel_path: Excelæ–‡ä»¶è·¯å¾„
            video_dir: è§†é¢‘ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            env_file: ç¯å¢ƒå˜é‡æ–‡ä»¶è·¯å¾„
        """
        self.data_path = data_path
        self.excel_path = excel_path
        self.video_dir = video_dir
        self.output_dir = output_dir
        self.env_file = env_file
        
        # è®¾ç½®ç¯å¢ƒ
        self._setup_environment()
        
        # åŠ è½½æ•°æ®
        self._load_data()
        
        # å®éªŒå‚æ•°
        self.sample_size = sample_size  # æŸ¥è¯¢æ ·æœ¬æ•°é‡
        self.k = 85  # top-k = 85 (æ•°æ®é›†çš„çº¦10%)
        self.m = 9  # å›ºå®šç°‡æ•°ç”¨äºåˆ†å¸ƒæµ‹é‡
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        logger.info(f"ç†µå€¼åˆ†å¸ƒåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æ ·æœ¬æ•°é‡: {self.sample_size}, k: {self.k}, m: {self.m}")
    
    def _setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        if os.path.exists(self.env_file):
            load_env_variables(self.env_file)
            logger.info(f"å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {self.env_file}")
        else:
            logger.warning(f"ç¯å¢ƒå˜é‡æ–‡ä»¶ä¸å­˜åœ¨: {self.env_file}")
        
        # éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
        try:
            self.project_id = get_required_env("GOOGLE_CLOUD_PROJECT_ID")
            self.location = get_required_env("GOOGLE_CLOUD_LOCATION")
            logger.info(f"Google Cloud é…ç½®: Project ID = {self.project_id}, Location = {self.location}")
        except Exception as e:
            logger.error(f"ç¼ºå°‘å¿…è¦çš„ Google Cloud ç¯å¢ƒå˜é‡: {str(e)}")
            raise
    
    def _load_data(self):
        """åŠ è½½MAFWæ•°æ®é›†"""
        logger.info(f"åŠ è½½MAFWæ•°æ®é›†: {self.excel_path}")
        
        # åŠ è½½Excelæ•°æ®
        if self.excel_path and os.path.exists(self.excel_path):
            self.df = pd.read_excel(self.excel_path)
            logger.info(f"åŠ è½½äº† {len(self.df)} æ¡æ•°æ®è®°å½•")
        else:
            raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {self.excel_path}")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ['video_name', 'eng_caption']
        missing_columns = [col for col in required_columns if col not in self.df.columns]
        if missing_columns:
            raise ValueError(f"Excelæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
        
        # åˆ›å»ºæŸ¥è¯¢åˆ—è¡¨
        self.queries = []
        for _, row in self.df.iterrows():
            self.queries.append({
                "video": row['video_name'],
                "caption": row['eng_caption'],
                "video_path": os.path.join(self.video_dir, f"{row['video_name']}.mp4")
            })
        
        logger.info(f"åˆ›å»ºäº† {len(self.queries)} ä¸ªæŸ¥è¯¢")
    
    def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        logger.info("åˆå§‹åŒ– MERLIN ç»„ä»¶...")
        
        # åˆå§‹åŒ– Questionerï¼ˆç”¨äºç†µå€¼è®¡ç®—ï¼‰
        self.questioner = Questioner(
            n_clusters=self.m,
            alpha_threshold=0.5,  # ä¸´æ—¶å€¼ï¼Œä¸å½±å“ç†µå€¼è®¡ç®—
            beta_threshold=0.3    # ä¸´æ—¶å€¼ï¼Œä¸å½±å“ç†µå€¼è®¡ç®—
        )
        
        # åˆå§‹åŒ– Rerankerï¼ˆä½¿ç”¨ Google embeddingï¼‰
        try:
            self.reranker = Reranker(
                location=self.location,
                project_id=self.project_id,
                memory_path=self.data_path,
                queries=self.queries,
                video_ext=".mp4"
            )
            logger.info("âœ… æˆåŠŸåˆå§‹åŒ–çœŸæ­£çš„ Google embedding Reranker")
        except Exception as e:
            logger.error(f"âŒ Reranker åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _get_zero_shot_ranking(self, query_text_emb: np.ndarray, video_embs: List[np.ndarray], top_k: int) -> List[int]:
        """é›¶æ ·æœ¬æ£€ç´¢æ’å"""
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for video_emb in video_embs:
            similarity = np.dot(query_text_emb, video_emb) / (np.linalg.norm(query_text_emb) * np.linalg.norm(video_emb))
            similarities.append(similarity)
        
        # è·å–top-kç´¢å¼•
        top_indices = np.argsort(similarities)[::-1][:top_k]
        return top_indices.tolist()
    
    def _load_video_embeddings(self) -> List[np.ndarray]:
        """åŠ è½½æ‰€æœ‰è§†é¢‘çš„åµŒå…¥å‘é‡"""
        logger.info("åŠ è½½è§†é¢‘åµŒå…¥å‘é‡...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„åµŒå…¥
        embedding_dir = Path(self.data_path) / "video_embeddings"
        if embedding_dir.exists():
            # åŠ è½½é¢„è®¡ç®—çš„åµŒå…¥
            video_embs = []
            valid_queries = []
            for query in self.queries:
                video_name = query["video"]
                video_id = video_name.replace('.mp4', '')  # å»æ‰.mp4åç¼€
                emb_file = embedding_dir / f"{video_id}.npy"
                if emb_file.exists():
                    emb = np.load(str(emb_file))
                    video_embs.append(emb)
                    valid_queries.append(query)
                else:
                    logger.warning(f"è·³è¿‡è§†é¢‘ {video_name}ï¼šç¼ºå°‘è§†é¢‘åµŒå…¥æ–‡ä»¶")
            
            # æ›´æ–°queriesåˆ—è¡¨ï¼Œåªä¿ç•™æœ‰æ•ˆçš„æŸ¥è¯¢
            self.queries = valid_queries
            logger.info(f"åŠ è½½äº† {len(video_embs)} ä¸ªæœ‰æ•ˆè§†é¢‘åµŒå…¥")
            return video_embs
        else:
            logger.warning("æœªæ‰¾åˆ°é¢„è®¡ç®—çš„åµŒå…¥ï¼Œå°†ä½¿ç”¨éšæœºå‘é‡")
            # ä½¿ç”¨éšæœºå‘é‡ä½œä¸ºå ä½ç¬¦
            return [np.random.randn(1408) for _ in self.queries]
    
    def _load_text_embeddings(self) -> List[np.ndarray]:
        """åŠ è½½æ‰€æœ‰æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        logger.info("åŠ è½½æ–‡æœ¬åµŒå…¥å‘é‡...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„åµŒå…¥
        embedding_dir = Path(self.data_path) / "text_embeddings"
        if embedding_dir.exists():
            # åŠ è½½é¢„è®¡ç®—çš„åµŒå…¥
            text_embs = []
            for query in self.queries:
                video_name = query["video"]
                video_id = video_name.replace('.mp4', '')  # å»æ‰.mp4åç¼€
                emb_file = embedding_dir / f"{video_id}.npy"
                if emb_file.exists():
                    emb = np.load(str(emb_file))
                    text_embs.append(emb)
                else:
                    logger.warning(f"è·³è¿‡è§†é¢‘ {video_name}ï¼šç¼ºå°‘æ–‡æœ¬åµŒå…¥æ–‡ä»¶")
                    # ä½¿ç”¨éšæœºå‘é‡ä½œä¸ºå ä½ç¬¦
                    text_embs.append(np.random.randn(1408))
            
            logger.info(f"åŠ è½½äº† {len(text_embs)} ä¸ªæ–‡æœ¬åµŒå…¥")
            return text_embs
        else:
            logger.warning("æœªæ‰¾åˆ°é¢„è®¡ç®—çš„åµŒå…¥ï¼Œå°†ä½¿ç”¨éšæœºå‘é‡")
            # ä½¿ç”¨éšæœºå‘é‡ä½œä¸ºå ä½ç¬¦
            return [np.random.randn(1408) for _ in self.queries]
    
    def measure_entropy_distribution(self) -> pd.DataFrame:
        """æµ‹é‡ç†µå€¼åˆ†å¸ƒ"""
        logger.info("å¼€å§‹ç†µå€¼åˆ†å¸ƒæµ‹é‡...")
        
        # åŠ è½½åµŒå…¥å‘é‡
        video_embs = self._load_video_embeddings()
        text_embs = self._load_text_embeddings()
        
        # ç¡®ä¿æ ·æœ¬æ•°é‡ä¸è¶…è¿‡æŸ¥è¯¢æ•°é‡
        actual_sample_size = min(self.sample_size, len(self.queries))
        logger.info(f"æŸ¥è¯¢æ•°é‡: {len(self.queries)}, å®é™…é‡‡æ ·æ•°é‡: {actual_sample_size}")
        
        # éšæœºé€‰æ‹©æŸ¥è¯¢æ ·æœ¬
        random.seed(42)
        sample_indices = random.sample(range(len(self.queries)), actual_sample_size)
        
        # è®°å½•ç»“æœ
        results = []
        
        for i, query_idx in enumerate(tqdm(sample_indices, desc="æµ‹é‡ç†µå€¼åˆ†å¸ƒ")):
            try:
                # è·å–æŸ¥è¯¢ä¿¡æ¯
                query = self.queries[query_idx]
                query_text_emb = text_embs[query_idx]
                
                # åˆ›å»ºå€™é€‰è§†é¢‘åˆ—è¡¨ï¼ˆæ’é™¤æŸ¥è¯¢è§†é¢‘æœ¬èº«ï¼‰
                candidate_indices = [j for j in range(len(self.queries)) if j != query_idx]
                candidate_video_embs = [video_embs[j] for j in candidate_indices]
                
                # é›¶æ ·æœ¬æ£€ç´¢ï¼Œè·å–top-kå€™é€‰
                top_k_indices = self._get_zero_shot_ranking(query_text_emb, candidate_video_embs, self.k)
                
                # è·å–top-kå€™é€‰çš„åµŒå…¥å‘é‡
                top_k_embeddings = [candidate_video_embs[idx] for idx in top_k_indices]
                embeddings_array = np.array(top_k_embeddings)
                
                # è®¡ç®—ç†µå€¼
                inter_cluster_entropy, intra_cluster_entropy, cluster_info = self.questioner.compute_entropy_metrics(embeddings_array)
                
                # è®°å½•ç»“æœ
                result = {
                    'Query_ID': f"Sample_{i+1:03d}",
                    'k': self.k,
                    'm': self.m,
                    'Calculated_Inter_Cluster_Entropy': inter_cluster_entropy,
                    'Calculated_Intra_Cluster_Entropy': intra_cluster_entropy,
                    'query_video': query["video"],
                    'query_caption': query["caption"][:100] + "..." if len(query["caption"]) > 100 else query["caption"]
                }
                results.append(result)
                
                logger.debug(f"æ ·æœ¬ {i+1}: ç°‡é—´ç†µ={inter_cluster_entropy:.4f}, ç°‡å†…ç†µ={intra_cluster_entropy:.4f}")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ ·æœ¬ {i+1} å¤±è´¥: {str(e)}")
                continue
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(results)
        
        # ä¿å­˜ç»“æœ
        self._save_entropy_results(df)
        
        return df
    
    def _save_entropy_results(self, df: pd.DataFrame):
        """ä¿å­˜ç†µå€¼åˆ†å¸ƒç»“æœ"""
        output_path = Path(self.output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜CSV
        csv_path = output_path / "entropy_distribution_results.csv"
        df.to_csv(csv_path, index=False)
        logger.info(f"ç†µå€¼åˆ†å¸ƒç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
        
        # ä¿å­˜JSONï¼ˆç”¨äºåç»­åˆ†æï¼‰
        json_path = output_path / "entropy_distribution_results.json"
        df.to_json(json_path, orient='records', indent=2)
        logger.info(f"ç†µå€¼åˆ†å¸ƒç»“æœå·²ä¿å­˜åˆ°: {json_path}")
    
    def analyze_entropy_distribution(self, df: pd.DataFrame):
        """åˆ†æç†µå€¼åˆ†å¸ƒå¹¶ç”Ÿæˆå¯è§†åŒ–"""
        logger.info("åˆ†æç†µå€¼åˆ†å¸ƒ...")
        
        output_path = Path(self.output_dir)
        output_path.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('MAFW Dataset - Entropy Distribution Analysis Results', fontsize=16)
        
        # 1. ç°‡é—´ç†µåˆ†å¸ƒç›´æ–¹å›¾
        axes[0,0].hist(df['Calculated_Inter_Cluster_Entropy'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0,0].set_xlabel('Inter-Cluster Entropy (Î±)')
        axes[0,0].set_ylabel('Frequency')
        axes[0,0].set_title('Inter-Cluster Entropy Distribution')
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. å¹³å‡ç°‡å†…ç†µåˆ†å¸ƒç›´æ–¹å›¾
        axes[0,1].hist(df['Calculated_Intra_Cluster_Entropy'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')
        axes[0,1].set_xlabel('Intra-Cluster Entropy (Î²)')
        axes[0,1].set_ylabel('Frequency')
        axes[0,1].set_title('Intra-Cluster Entropy Distribution')
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. æ•£ç‚¹å›¾ï¼šç°‡é—´ç†µ vs ç°‡å†…ç†µ
        scatter = axes[1,0].scatter(df['Calculated_Inter_Cluster_Entropy'], 
                                  df['Calculated_Intra_Cluster_Entropy'], 
                                  alpha=0.6, s=50)
        axes[1,0].set_xlabel('Inter-Cluster Entropy (Î±)')
        axes[1,0].set_ylabel('Intra-Cluster Entropy (Î²)')
        axes[1,0].set_title('Inter-Cluster vs Intra-Cluster Entropy')
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. ç®±çº¿å›¾
        data_for_box = [df['Calculated_Inter_Cluster_Entropy'], df['Calculated_Intra_Cluster_Entropy']]
        axes[1,1].boxplot(data_for_box, labels=['Inter-Cluster Entropy (Î±)', 'Intra-Cluster Entropy (Î²)'])
        axes[1,1].set_ylabel('Entropy Value')
        axes[1,1].set_title('Entropy Box Plot')
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = output_path / "entropy_distribution_analysis.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        logger.info(f"ç†µå€¼åˆ†å¸ƒåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
        
        plt.show()
    
    def calculate_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡é‡"""
        logger.info("è®¡ç®—ç»Ÿè®¡é‡...")
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        alpha_stats = df['Calculated_Inter_Cluster_Entropy'].describe(percentiles=[0.25, 0.5, 0.75, 0.9])
        beta_stats = df['Calculated_Intra_Cluster_Entropy'].describe(percentiles=[0.25, 0.5, 0.75, 0.9])
        
        statistics = {
            'alpha_statistics': {
                'min': alpha_stats['min'],
                '25%': alpha_stats['25%'],
                '50%': alpha_stats['50%'],
                '75%': alpha_stats['75%'],
                '90%': alpha_stats['90%'],
                'max': alpha_stats['max'],
                'mean': alpha_stats['mean'],
                'std': alpha_stats['std']
            },
            'beta_statistics': {
                'min': beta_stats['min'],
                '25%': beta_stats['25%'],
                '50%': beta_stats['50%'],
                '75%': beta_stats['75%'],
                '90%': beta_stats['90%'],
                'max': beta_stats['max'],
                'mean': beta_stats['mean'],
                'std': beta_stats['std']
            }
        }
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        output_path = Path(self.output_dir)
        output_path.mkdir(exist_ok=True)
        
        stats_path = output_path / "entropy_statistics.json"
        with open(stats_path, 'w') as f:
            json.dump(statistics, f, indent=2)
        logger.info(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {stats_path}")
        
        return statistics
    
    def suggest_parameter_ranges(self, statistics: Dict[str, Any]) -> Dict[str, List[float]]:
        """åŸºäºç»Ÿè®¡ç»“æœå»ºè®®å‚æ•°æœç´¢èŒƒå›´"""
        logger.info("åŸºäºç»Ÿè®¡ç»“æœå»ºè®®å‚æ•°æœç´¢èŒƒå›´...")
        
        alpha_stats = statistics['alpha_statistics']
        beta_stats = statistics['beta_statistics']
        
        # åŸºäºç™¾åˆ†ä½æ•°ç¡®å®šæœç´¢èŒƒå›´
        # å¯¹äº Alphaï¼Œä½¿ç”¨æ›´å°çš„åç§»é‡ï¼ˆå› ä¸ºå€¼åŸŸè¾ƒå°ï¼‰
        alpha_offset = min(0.001, alpha_stats['std'] * 2)  # ä½¿ç”¨æ ‡å‡†å·®æˆ–0.001çš„è¾ƒå°å€¼
        alpha_range = [
            round(alpha_stats['25%'] - alpha_offset, 4),  # 25%ç™¾åˆ†ä½æ•°é™„è¿‘
            round(alpha_stats['25%'], 4),
            round(alpha_stats['50%'], 4),                 # 50%ç™¾åˆ†ä½æ•°
            round(alpha_stats['75%'], 4),                 # 75%ç™¾åˆ†ä½æ•°
            round(alpha_stats['90%'], 4),                 # 90%ç™¾åˆ†ä½æ•°
            round(alpha_stats['90%'] + alpha_offset, 4)   # 90%ç™¾åˆ†ä½æ•°é™„è¿‘
        ]
        
        # å¯¹äº Betaï¼Œä½¿ç”¨ç›¸å¯¹åç§»é‡
        beta_offset = min(0.01, beta_stats['std'] * 2)  # ä½¿ç”¨æ ‡å‡†å·®æˆ–0.01çš„è¾ƒå°å€¼
        beta_range = [
            round(beta_stats['25%'] - beta_offset, 3),   # 25%ç™¾åˆ†ä½æ•°é™„è¿‘
            round(beta_stats['25%'], 3),
            round(beta_stats['50%'], 3),                 # 50%ç™¾åˆ†ä½æ•°
            round(beta_stats['75%'], 3),                 # 75%ç™¾åˆ†ä½æ•°
            round(beta_stats['90%'], 3),                 # 90%ç™¾åˆ†ä½æ•°
            round(beta_stats['90%'] + beta_offset, 3)    # 90%ç™¾åˆ†ä½æ•°é™„è¿‘
        ]
        
        # å»é‡å¹¶æ’åº
        alpha_range = sorted(list(set(alpha_range)))
        beta_range = sorted(list(set(beta_range)))
        
        # mçš„èŒƒå›´ï¼ˆåŸºäºå®éªŒè®¾è®¡ï¼‰
        m_range = [4, 6, 8, 10, 12]
        
        suggested_ranges = {
            'm': m_range,
            'alpha': alpha_range,
            'beta': beta_range
        }
        
        # ä¿å­˜å»ºè®®çš„å‚æ•°èŒƒå›´
        output_path = Path(self.output_dir)
        output_path.mkdir(exist_ok=True)
        
        ranges_path = output_path / "suggested_parameter_ranges.json"
        with open(ranges_path, 'w') as f:
            json.dump(suggested_ranges, f, indent=2)
        logger.info(f"å»ºè®®çš„å‚æ•°èŒƒå›´å·²ä¿å­˜åˆ°: {ranges_path}")
        
        return suggested_ranges
    
    def run_entropy_analysis(self):
        """è¿è¡Œå®Œæ•´çš„ç†µå€¼åˆ†ææµç¨‹"""
        logger.info("å¼€å§‹å®Œæ•´çš„ç†µå€¼åˆ†å¸ƒåˆ†ææµç¨‹...")
        
        # 1. æµ‹é‡ç†µå€¼åˆ†å¸ƒ
        df = self.measure_entropy_distribution()
        
        # 2. åˆ†æç†µå€¼åˆ†å¸ƒ
        self.analyze_entropy_distribution(df)
        
        # 3. è®¡ç®—ç»Ÿè®¡é‡
        statistics = self.calculate_statistics(df)
        
        # 4. å»ºè®®å‚æ•°æœç´¢èŒƒå›´
        suggested_ranges = self.suggest_parameter_ranges(statistics)
        
        # 5. æ‰“å°æ€»ç»“
        self._print_entropy_summary(df, statistics, suggested_ranges)
    
    def _print_entropy_summary(self, df: pd.DataFrame, statistics: Dict[str, Any], suggested_ranges: Dict[str, List[float]]):
        """æ‰“å°ç†µå€¼åˆ†ææ€»ç»“"""
        print("\n" + "="*60)
        print("MAFW æ•°æ®é›†ç†µå€¼åˆ†å¸ƒåˆ†ææ€»ç»“")
        print("="*60)
        
        print(f"ğŸ“Š åˆ†ææ ·æœ¬æ•°é‡: {len(df)}")
        print(f"ğŸ”¢ å›ºå®šå‚æ•°: k={self.k}, m={self.m}")
        
        print(f"\nğŸ“ˆ ç°‡é—´ç†µ (Î±) ç»Ÿè®¡:")
        alpha_stats = statistics['alpha_statistics']
        print(f"   - æœ€å°å€¼: {alpha_stats['min']:.4f}")
        print(f"   - 25%ç™¾åˆ†ä½æ•°: {alpha_stats['25%']:.4f}")
        print(f"   - 50%ç™¾åˆ†ä½æ•°: {alpha_stats['50%']:.4f}")
        print(f"   - 75%ç™¾åˆ†ä½æ•°: {alpha_stats['75%']:.4f}")
        print(f"   - 90%ç™¾åˆ†ä½æ•°: {alpha_stats['90%']:.4f}")
        print(f"   - æœ€å¤§å€¼: {alpha_stats['max']:.4f}")
        print(f"   - å‡å€¼: {alpha_stats['mean']:.4f}")
        print(f"   - æ ‡å‡†å·®: {alpha_stats['std']:.4f}")
        
        print(f"\nğŸ“ˆ å¹³å‡ç°‡å†…ç†µ (Î²) ç»Ÿè®¡:")
        beta_stats = statistics['beta_statistics']
        print(f"   - æœ€å°å€¼: {beta_stats['min']:.4f}")
        print(f"   - 25%ç™¾åˆ†ä½æ•°: {beta_stats['25%']:.4f}")
        print(f"   - 50%ç™¾åˆ†ä½æ•°: {beta_stats['50%']:.4f}")
        print(f"   - 75%ç™¾åˆ†ä½æ•°: {beta_stats['75%']:.4f}")
        print(f"   - 90%ç™¾åˆ†ä½æ•°: {beta_stats['90%']:.4f}")
        print(f"   - æœ€å¤§å€¼: {beta_stats['max']:.4f}")
        print(f"   - å‡å€¼: {beta_stats['mean']:.4f}")
        print(f"   - æ ‡å‡†å·®: {beta_stats['std']:.4f}")
        
        print(f"\nğŸ¯ å»ºè®®çš„å‚æ•°æœç´¢èŒƒå›´:")
        print(f"   - m: {suggested_ranges['m']}")
        print(f"   - Î±: {suggested_ranges['alpha']}")
        print(f"   - Î²: {suggested_ranges['beta']}")
        
        total_combinations = len(suggested_ranges['m']) * len(suggested_ranges['alpha']) * len(suggested_ranges['beta'])
        print(f"   - æ€»å‚æ•°ç»„åˆæ•°: {total_combinations}")
        
        print(f"\nğŸ’¾ ç»“æœæ–‡ä»¶:")
        print(f"   - ç†µå€¼åˆ†å¸ƒç»“æœ: {self.output_dir}/entropy_distribution_results.csv")
        print(f"   - ç»Ÿè®¡ç»“æœ: {self.output_dir}/entropy_statistics.json")
        print(f"   - å»ºè®®å‚æ•°èŒƒå›´: {self.output_dir}/suggested_parameter_ranges.json")
        print(f"   - å¯è§†åŒ–å›¾è¡¨: {self.output_dir}/entropy_distribution_analysis.png")
        print("="*60)

def main():
    parser = argparse.ArgumentParser(description="MAFW æ•°æ®é›†ç†µå€¼åˆ†å¸ƒåˆ†æ - ä¸ºå‚æ•°è°ƒä¼˜åšå‡†å¤‡")
    
    parser.add_argument("--data_path", type=str, default="/home/peterchen/M2/ADEPT/data/mafw",
                       help="æ•°æ®è·¯å¾„ (é»˜è®¤: /home/peterchen/M2/ADEPT/data/mafw)")
    parser.add_argument("--excel_path", type=str, default="/home/peterchen/M2/ADEPT/data/mafw/labels/sampled_850.xlsx",
                       help="Excelæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--video_dir", type=str, default="/home/peterchen/M2/ADEPT/data/mafw/videos",
                       help="è§†é¢‘ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="/home/peterchen/M2/ADEPT/data/mafw/entropy_analysis_outputs",
                       help="è¾“å‡ºç›®å½• (é»˜è®¤: /home/peterchen/M2/ADEPT/data/mafw/entropy_analysis_outputs)")
    parser.add_argument("--env_file", type=str, default="/home/peterchen/M2/ADEPT/.env",
                       help="ç¯å¢ƒå˜é‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample_size", type=int, default=400,
                       help="æ ·æœ¬æ•°é‡ (é»˜è®¤: 400)")
    
    args = parser.parse_args()
    
    print(f"ğŸ”¬ å¼€å§‹ MAFW æ•°æ®é›†ç†µå€¼åˆ†å¸ƒåˆ†æ")
    print(f"ğŸ“Š æ•°æ®é›†: MAFW")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {args.sample_size}")
    print(f"ğŸ”§ å°†ä½¿ç”¨çœŸæ­£çš„ Google Multimodal Embedding API")
    
    try:
        # åˆ›å»ºç†µå€¼åˆ†å¸ƒåˆ†æå™¨
        analyzer = EntropyDistributionAnalyzer(
            data_path=args.data_path,
            excel_path=args.excel_path,
            video_dir=args.video_dir,
            output_dir=args.output_dir,
            env_file=args.env_file,
            sample_size=args.sample_size
        )
        
        # è¿è¡Œç†µå€¼åˆ†ææµç¨‹
        analyzer.run_entropy_analysis()
        
        print("âœ… MAFW æ•°æ®é›†ç†µå€¼åˆ†å¸ƒåˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {args.output_dir}/")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†åˆ†æè¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç†µå€¼åˆ†å¸ƒåˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 