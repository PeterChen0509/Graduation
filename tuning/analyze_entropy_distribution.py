#!/usr/bin/env python3
"""
åˆ†æå®é™…çš„alphaå’Œbetaåˆ†å¸ƒ
ä½¿ç”¨æ›´å¤§çš„må€¼å’Œtop_kæ¥æ¨¡æ‹Ÿå®Œæ•´æ•°æ®é›†çš„æƒ…å†µ
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
import random
from typing import List, Dict, Any
import matplotlib.pyplot as plt
import seaborn as sns

# è‡ªåŠ¨è®¾ç½®PYTHONPATH
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.logger import logger, setup_logger
from utils.env_utils import load_env_variables
from utils.data_utils import DatasetConfig, DATASET_CONFIGS
from merlin.questioner import Questioner

class EntropyAnalyzer:
    """åˆ†æå®é™…çš„ç†µå€¼åˆ†å¸ƒ"""
    
    def __init__(self, 
                 data_path: str = "/home/peterchen/M2/ADEPT/data/mafw",
                 excel_path: str = "/home/peterchen/M2/ADEPT/data/mafw/labels/sampled_850.xlsx",
                 env_file: str = "/home/peterchen/M2/ADEPT/.env"):
        
        self.data_path = data_path
        self.excel_path = excel_path
        self.env_file = env_file
        
        # è®¾ç½®ç¯å¢ƒ
        setup_logger()
        if os.path.exists(self.env_file):
            load_env_variables(self.env_file)
        
        # åŠ è½½æ•°æ®
        self._load_data()
        
        # æµ‹è¯•å‚æ•°
        self.test_m_values = [8, 12]  # åªæµ‹ä¸¤ä¸ªmå€¼
        self.test_top_k = 85  # æ¨¡æ‹Ÿå®Œæ•´æ•°æ®é›†çš„top_k
        
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        logger.info("åŠ è½½æ•°æ®...")
        
        # åŠ è½½Excelæ•°æ®
        self.df = pd.read_excel(self.excel_path)
        logger.info(f"åŠ è½½äº† {len(self.df)} æ¡æ•°æ®è®°å½•")
        
        # åŠ è½½åµŒå…¥å‘é‡
        embedding_base_path = Path(self.data_path)
        video_emb_dir = embedding_base_path / "video_embeddings"
        text_emb_dir = embedding_base_path / "text_embeddings"
        
        # åŠ è½½è§†é¢‘åµŒå…¥
        video_emb_files = list(video_emb_dir.glob("*.npy"))
        self.video_embs = []
        self.valid_video_ids = []
        for emb_file in sorted(video_emb_files):
            video_id = emb_file.stem
            emb = np.load(str(emb_file))
            self.video_embs.append(emb)
            self.valid_video_ids.append(video_id)
        
        # åŠ è½½æ–‡æœ¬åµŒå…¥
        text_emb_files = list(text_emb_dir.glob("*.npy"))
        self.text_embs = []
        self.valid_text_ids = []
        for emb_file in sorted(text_emb_files):
            text_id = emb_file.stem
            emb = np.load(str(emb_file))
            self.text_embs.append(emb)
            self.valid_text_ids.append(text_id)
        
        logger.info(f"åŠ è½½äº† {len(self.video_embs)} ä¸ªè§†é¢‘åµŒå…¥")
        logger.info(f"åŠ è½½äº† {len(self.text_embs)} ä¸ªæ–‡æœ¬åµŒå…¥")
        
        # åˆ›å»ºæŸ¥è¯¢åˆ—è¡¨
        self.queries = []
        for idx, row in self.df.iterrows():
            video_name = row['video_name']
            video_id = video_name.replace('.mp4', '')
            
            if video_id in self.valid_video_ids and video_id in self.valid_text_ids:
                self.queries.append({
                    'video': video_name,
                    'text': row['eng_caption'],
                    'video_id': video_id
                })
        
        logger.info(f"åˆ›å»ºäº† {len(self.queries)} ä¸ªæœ‰æ•ˆæŸ¥è¯¢")
    
    def _get_zero_shot_ranking(self, query_text_emb: np.ndarray, video_embs: List[np.ndarray], top_k: int) -> List[int]:
        """é›¶æ ·æœ¬æ£€ç´¢æ’å"""
        similarities = []
        for video_emb in video_embs:
            similarity = np.dot(query_text_emb, video_emb) / (np.linalg.norm(query_text_emb) * np.linalg.norm(video_emb))
            similarities.append(similarity)
        
        top_indices = np.argsort(similarities)[::-1][:top_k]
        return top_indices.tolist()
    
    def analyze_entropy_distribution(self, sample_size: int = 50):
        """åˆ†æç†µå€¼åˆ†å¸ƒ"""
        logger.info(f"å¼€å§‹åˆ†æç†µå€¼åˆ†å¸ƒï¼Œæ ·æœ¬æ•°é‡: {sample_size}")
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        random.seed(42)
        selected_indices = random.sample(range(len(self.queries)), min(sample_size, len(self.queries)))
        
        results = []
        
        for m in self.test_m_values:
            logger.info(f"æµ‹è¯• m={m}")
            
            for idx in selected_indices:
                query = self.queries[idx]
                query_text_emb = self.text_embs[self.valid_text_ids.index(query['video_id'])]
                
                # è·å–top_kæ’å
                top_k_indices = self._get_zero_shot_ranking(query_text_emb, self.video_embs, self.test_top_k)
                top_k_embeddings = [self.video_embs[i] for i in top_k_indices]
                
                if len(top_k_embeddings) < m:
                    logger.warning(f"æ ·æœ¬æ•°é‡ {len(top_k_embeddings)} å°äºç°‡æ•° {m}ï¼Œè·³è¿‡")
                    continue
                
                # è®¡ç®—ç†µå€¼
                try:
                    questioner = Questioner(
                        n_clusters=m,
                        alpha_threshold=0.01,  # ä¸´æ—¶å€¼
                        beta_threshold=0.01    # ä¸´æ—¶å€¼
                    )
                    
                    # è®¡ç®—ç†µå€¼
                    embeddings_array = np.array(top_k_embeddings)
                    inter_cluster_entropy, intra_cluster_entropy, cluster_info = questioner.compute_entropy_metrics(embeddings_array)
                    
                    results.append({
                        'm': m,
                        'query_idx': idx,
                        'video_id': query['video_id'],
                        'inter_cluster_entropy': inter_cluster_entropy,
                        'intra_cluster_entropy': intra_cluster_entropy,
                        'top_k': len(top_k_embeddings)
                    })
                    
                except Exception as e:
                    logger.error(f"è®¡ç®—ç†µå€¼å¤±è´¥: {str(e)}")
                    continue
        
        return results
    
    def save_results(self, results: List[Dict], output_dir: str = "entropy_analysis"):
        """ä¿å­˜ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(results)
        
        # ä¿å­˜CSV
        df.to_csv(output_path / "entropy_analysis_results.csv", index=False)
        
        # è®¡ç®—ç»Ÿè®¡é‡ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªå…³æ³¨å…³é”®ç™¾åˆ†ä½æ•°
        stats = {}
        
        # åˆå¹¶æ‰€æœ‰må€¼çš„æ•°æ®ï¼Œè®¡ç®—æ€»ä½“åˆ†å¸ƒ
        if not df.empty:
            stats['alpha_statistics'] = {
                'min': df['inter_cluster_entropy'].min(),
                '25%': df['inter_cluster_entropy'].quantile(0.25),
                '50%': df['inter_cluster_entropy'].quantile(0.5),
                '75%': df['inter_cluster_entropy'].quantile(0.75),
                '90%': df['inter_cluster_entropy'].quantile(0.9),
                'max': df['inter_cluster_entropy'].max(),
                'mean': df['inter_cluster_entropy'].mean(),
                'std': df['inter_cluster_entropy'].std()
            }
            
            stats['beta_statistics'] = {
                'min': df['intra_cluster_entropy'].min(),
                '25%': df['intra_cluster_entropy'].quantile(0.25),
                '50%': df['intra_cluster_entropy'].quantile(0.5),
                '75%': df['intra_cluster_entropy'].quantile(0.75),
                '90%': df['intra_cluster_entropy'].quantile(0.9),
                'max': df['intra_cluster_entropy'].max(),
                'mean': df['intra_cluster_entropy'].mean(),
                'std': df['intra_cluster_entropy'].std()
            }
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        import json
        with open(output_path / "entropy_statistics.json", 'w') as f:
            json.dump(stats, f, indent=2, default=str)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return stats
    
    def _create_visualizations(self, df: pd.DataFrame, output_path: Path):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ç°‡é—´ç†µåˆ†å¸ƒï¼ˆæŒ‰må€¼ï¼‰
        for m in self.test_m_values:
            m_data = df[df['m'] == m]
            if not m_data.empty:
                axes[0,0].hist(m_data['inter_cluster_entropy'], alpha=0.7, label=f'm={m}', bins=20)
        axes[0,0].set_xlabel('ç°‡é—´ç†µ (Î±)')
        axes[0,0].set_ylabel('é¢‘æ¬¡')
        axes[0,0].set_title('ç°‡é—´ç†µåˆ†å¸ƒï¼ˆæŒ‰må€¼ï¼‰')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. ç°‡å†…ç†µåˆ†å¸ƒï¼ˆæŒ‰må€¼ï¼‰
        for m in self.test_m_values:
            m_data = df[df['m'] == m]
            if not m_data.empty:
                axes[0,1].hist(m_data['intra_cluster_entropy'], alpha=0.7, label=f'm={m}', bins=20)
        axes[0,1].set_xlabel('ç°‡å†…ç†µ (Î²)')
        axes[0,1].set_ylabel('é¢‘æ¬¡')
        axes[0,1].set_title('ç°‡å†…ç†µåˆ†å¸ƒï¼ˆæŒ‰må€¼ï¼‰')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. ç®±çº¿å›¾ - ç°‡é—´ç†µ
        inter_data = [df[df['m'] == m]['inter_cluster_entropy'].values for m in self.test_m_values if not df[df['m'] == m].empty]
        axes[1,0].boxplot(inter_data, labels=[f'm={m}' for m in self.test_m_values if not df[df['m'] == m].empty])
        axes[1,0].set_xlabel('må€¼')
        axes[1,0].set_ylabel('ç°‡é—´ç†µ (Î±)')
        axes[1,0].set_title('ç°‡é—´ç†µç®±çº¿å›¾')
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. ç®±çº¿å›¾ - ç°‡å†…ç†µ
        intra_data = [df[df['m'] == m]['intra_cluster_entropy'].values for m in self.test_m_values if not df[df['m'] == m].empty]
        axes[1,1].boxplot(intra_data, labels=[f'm={m}' for m in self.test_m_values if not df[df['m'] == m].empty])
        axes[1,1].set_xlabel('må€¼')
        axes[1,1].set_ylabel('ç°‡å†…ç†µ (Î²)')
        axes[1,1].set_title('ç°‡å†…ç†µç®±çº¿å›¾')
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path / "entropy_analysis_visualization.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def print_summary(self, stats: Dict):
        """æ‰“å°åˆ†ææ€»ç»“"""
        print("\n" + "="*60)
        print("ç†µå€¼åˆ†å¸ƒåˆ†ææ€»ç»“")
        print("="*60)
        print(f"æµ‹è¯•å‚æ•°: top_k={self.test_top_k}, må€¼={self.test_m_values}")
        
        if 'alpha_statistics' in stats:
            alpha_stats = stats['alpha_statistics']
            beta_stats = stats['beta_statistics']
            
            print(f"\nğŸ“Š Alpha (ç°‡é—´ç†µ) ç»Ÿè®¡ç»“æœ:")
            print(f"  æœ€å°å€¼: {alpha_stats['min']:.6f}")
            print(f"  25%åˆ†ä½æ•°: {alpha_stats['25%']:.6f}")
            print(f"  50%åˆ†ä½æ•°: {alpha_stats['50%']:.6f}")
            print(f"  75%åˆ†ä½æ•°: {alpha_stats['75%']:.6f}")
            print(f"  90%åˆ†ä½æ•°: {alpha_stats['90%']:.6f}")
            print(f"  æœ€å¤§å€¼: {alpha_stats['max']:.6f}")
            print(f"  å‡å€¼: {alpha_stats['mean']:.6f}")
            print(f"  æ ‡å‡†å·®: {alpha_stats['std']:.6f}")
            
            print(f"\nğŸ“Š Beta (ç°‡å†…ç†µ) ç»Ÿè®¡ç»“æœ:")
            print(f"  æœ€å°å€¼: {beta_stats['min']:.6f}")
            print(f"  25%åˆ†ä½æ•°: {beta_stats['25%']:.6f}")
            print(f"  50%åˆ†ä½æ•°: {beta_stats['50%']:.6f}")
            print(f"  75%åˆ†ä½æ•°: {beta_stats['75%']:.6f}")
            print(f"  90%åˆ†ä½æ•°: {beta_stats['90%']:.6f}")
            print(f"  æœ€å¤§å€¼: {beta_stats['max']:.6f}")
            print(f"  å‡å€¼: {beta_stats['mean']:.6f}")
            print(f"  æ ‡å‡†å·®: {beta_stats['std']:.6f}")
            
            print(f"\nğŸ’¡ æ¨èå‚æ•°èŒƒå›´:")
            print(f"  Alpha: [{alpha_stats['25%']:.6f}, {alpha_stats['50%']:.6f}, {alpha_stats['75%']:.6f}]")
            print(f"  Beta: [{beta_stats['25%']:.6f}, {beta_stats['50%']:.6f}, {beta_stats['75%']:.6f}]")
        
        print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•°"""
    analyzer = EntropyAnalyzer()
    
    # åˆ†æç†µå€¼åˆ†å¸ƒ
    results = analyzer.analyze_entropy_distribution(sample_size=10)
    
    # ä¿å­˜ç»“æœ
    stats = analyzer.save_results(results)
    
    # æ‰“å°æ€»ç»“
    analyzer.print_summary(stats)


if __name__ == "__main__":
    main() 