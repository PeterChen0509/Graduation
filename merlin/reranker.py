import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
from typing import Optional, List, Dict, Any
import vertexai
from vertexai.vision_models import (
    Image,
    MultiModalEmbeddingModel,
    MultiModalEmbeddingResponse,
    Video,
    VideoSegmentConfig,
)
from glob import glob
import os
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoTokenizer, AutoModel
import logging
import time


class Reformatter:
    """
    é‡æ„å™¨ (Reformatter) - è´Ÿè´£å°†å¯¹è¯æ—¥å¿—é‡æ„ä¸ºç²¾ç¡®çš„æè¿°æ–‡æœ¬
    
    è¿™ä¸ªæ¨¡å—æ˜¯æå‡æ£€ç´¢ç²¾åº¦çš„å…³é”®ã€‚å®ƒä¸æ˜¯ç®€å•çš„"æ€»ç»“"ï¼Œè€Œæ˜¯"è¿­ä»£å¼çš„ä¿¡æ¯æ•´åˆä¸é‡æ„"ã€‚
    æˆ‘ä»¬ä¸å¸Œæœ›ä¸¢å¤±ä»»ä½•ç»†èŠ‚ï¼Œè€Œæ˜¯å°†æ–°çš„é—®ç­”ä¿¡æ¯æ•´åˆåˆ°ç°æœ‰æè¿°ä¸­ã€‚
    """
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-VL-7B-Instruct", temperature: float = 0.1):
        """
        åˆå§‹åŒ–é‡æ„å™¨
        
        Args:
            model_name: Qwenæ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦å‚æ•°
        """
        self.logger = logging.getLogger(__name__)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½Qwen 2.5 VLæ¨¡å‹å’Œå¤„ç†å™¨
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.temperature = temperature
        
        # é‡æ„å™¨ä¸“ç”¨ç³»ç»Ÿæç¤º
        self.reformatter_system_prompt = {
            "role": "system",
            "content": """You are an expert in synthesizing information, acting as a 'Dialogue Reformatter'. Your task is to analyze a video search dialogue history and integrate all confirmed facts into a single, cohesive, descriptive paragraph.

Follow these rules strictly:
1. Start with the last reformulated description as your base.
2. Incorporate the new information from the latest Question-Answer pair to enrich the description.
3. The final output must be a single paragraph, written in a clear, objective, and descriptive style, perfect for a text-to-video retrieval system.
4. DO NOT include conversational elements, questions, uncertainties, or filler words. Only output the final, enriched description.
5. Maintain all previously confirmed details while adding new information.
6. Use precise, descriptive language that would be effective for video retrieval."""
        }
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        self.messages = []
        self.current_description = ""
    
    def reset_reformatter(self, initial_description: str = ""):
        """
        é‡ç½®é‡æ„å™¨çŠ¶æ€
        
        Args:
            initial_description: åˆå§‹æè¿°æ–‡æœ¬
        """
        self.messages = [self.reformatter_system_prompt]
        self.current_description = initial_description
        self.logger.debug(f"Reformatter reset with initial description: {initial_description}")
    
    def reformat_dialogue(self, question: str, answer: str, max_tokens: int = 500) -> str:
        """
        é‡æ„å¯¹è¯æ—¥å¿—ï¼Œç”Ÿæˆæ–°çš„æè¿°æ–‡æœ¬
        
        Args:
            question: å½“å‰è½®æ¬¡çš„é—®é¢˜
            answer: å½“å‰è½®æ¬¡çš„ç­”æ¡ˆ
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            é‡æ„åçš„æè¿°æ–‡æœ¬
        """
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        formatted_log = self._format_dialogue_history(question, answer)
        
        # åˆ›å»ºé‡æ„æç¤º
        user_message = f"""---
**Dialogue History to Reformulate:**
{formatted_log}
---

**New Reformulated Description:**"""
        
        # ç”Ÿæˆé‡æ„åçš„æè¿°
        new_description = self._generate_reformatted_description(user_message, max_tokens)
        
        # æˆªæ–­æè¿°æ–‡æœ¬ä»¥ç¡®ä¿ä¸è¶…è¿‡Google APIé™åˆ¶
        MAX_TEXT_LEN = 900  # è¿›ä¸€æ­¥å‡å°‘ï¼Œç¡®ä¿å®‰å…¨è¾¹ç•Œ
        if len(new_description) > MAX_TEXT_LEN:
            new_description = new_description[:MAX_TEXT_LEN]
            self.logger.warning(f"Description truncated from {len(new_description)} to {MAX_TEXT_LEN} characters")
        
        # æ›´æ–°å½“å‰æè¿°
        self.current_description = new_description
        
        # è®°å½•åˆ°å¯¹è¯å†å²
        self.messages.append({"role": "user", "content": user_message})
        self.messages.append({"role": "assistant", "content": new_description})
        
        self.logger.debug(f"Generated reformatted description: {new_description}")
        return new_description
    
    def _format_dialogue_history(self, question: str, answer: str) -> str:
        """
        æ ¼å¼åŒ–å¯¹è¯å†å²
        
        Args:
            question: å½“å‰é—®é¢˜
            answer: å½“å‰ç­”æ¡ˆ
            
        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯å†å²å­—ç¬¦ä¸²
        """
        if not self.current_description:
            # å¦‚æœæ²¡æœ‰ä¹‹å‰çš„æè¿°ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„åˆå§‹æè¿°
            self.current_description = "Initial video description"
        
        formatted_log = f"""Last Description: "{self.current_description}"
---
New Q&A Pair:
Question: "{question}"
Answer: "{answer}" """
        
        return formatted_log
    
    def _generate_reformatted_description(self, user_message: str, max_tokens: int) -> str:
        """
        ä½¿ç”¨Qwen2.5-VLç”Ÿæˆé‡æ„åçš„æè¿°
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            ç”Ÿæˆçš„æè¿°æ–‡æœ¬
        """
        try:
            # å¤„ç†è¾“å…¥
            text = self.processor.apply_chat_template(
                self.messages + [{"role": "user", "content": user_message}], 
                tokenize=False, 
                add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text],
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.device)

            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs, 
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=self.temperature
                )
            
            generated_ids_trimmed = [
                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            response = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0].strip()
            
            return response
            
        except Exception as e:
            self.logger.error(f"Error generating reformatted description: {str(e)}")
            # è¿”å›fallbackæè¿°
            return f"{self.current_description} {answer}"
    
    def get_current_description(self) -> str:
        """
        è·å–å½“å‰æè¿°æ–‡æœ¬
        
        Returns:
            å½“å‰æè¿°æ–‡æœ¬
        """
        return self.current_description
    
    def get_reformatter_history(self) -> List[Dict[str, str]]:
        """
        è·å–é‡æ„å™¨å†å²è®°å½•
        
        Returns:
            é‡æ„å™¨å†å²è®°å½•åˆ—è¡¨
        """
        return self.messages.copy()


class Reranker(torch.nn.Module):
    def __init__(self, location: str, project_id: str, memory_path: str, queries: list, video_ext: str = ".mp4"):
        super(Reranker, self).__init__()
        self.location = location
        self.project_id = project_id
        self.memory_path = memory_path
        self.video_ext = video_ext  # Store the video extension

        # Load queries
        self.memories = queries
        
        # åˆå§‹åŒ–é‡æ„å™¨
        self.reformatter = Reformatter()
        
        # åˆå§‹åŒ–fallbackæ–‡æœ¬embeddingæ¨¡å‹
        self._init_fallback_embedding_model()
    
    def _init_fallback_embedding_model(self):
        """åˆå§‹åŒ–fallbackæ–‡æœ¬embeddingæ¨¡å‹"""
        try:
            self.fallback_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
            self.fallback_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
            self.fallback_device = "cuda" if torch.cuda.is_available() else "cpu"
            self.fallback_model.to(self.fallback_device)
            self.fallback_model.eval()
            print(f"âœ… å·²åˆå§‹åŒ–fallbackæ–‡æœ¬embeddingæ¨¡å‹ (è®¾å¤‡: {self.fallback_device})")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åˆå§‹åŒ–fallbackæ–‡æœ¬embeddingæ¨¡å‹: {str(e)}")
            self.fallback_tokenizer = None
            self.fallback_model = None
    
    def _get_fallback_text_embedding(self, text: str) -> np.ndarray:
        """ä½¿ç”¨fallbackæ¨¡å‹è·å–æ–‡æœ¬embedding"""
        if self.fallback_model is None or self.fallback_tokenizer is None:
            raise Exception("Fallback embedding model not available")
        
        try:
            # ç¼–ç æ–‡æœ¬
            inputs = self.fallback_tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            ).to(self.fallback_device)
            
            # è·å–embedding
            with torch.no_grad():
                outputs = self.fallback_model(**inputs)
                # ä½¿ç”¨å¹³å‡æ± åŒ–
                embeddings = outputs.last_hidden_state.mean(dim=1)
                return embeddings.cpu().numpy()[0]
                
        except Exception as e:
            print(f"âŒ Fallback embeddingç”Ÿæˆå¤±è´¥: {str(e)}")
            # è¿”å›ä¸€ä¸ªéšæœºå‘é‡ä½œä¸ºæœ€åçš„fallback
            return np.random.randn(384)  # all-MiniLM-L6-v2çš„ç»´åº¦æ˜¯384

    def get_image_video_text_embeddings(
        self,
        image_path: Optional[str] = None, 
        video_path: Optional[str] = None,
        contextual_text: Optional[str] = None,
        dimension: Optional[int] = 1408,
        video_segment_config: Optional[VideoSegmentConfig] = None,
    ) -> MultiModalEmbeddingResponse:
        """Example of how to generate multimodal embeddings from image, video, and text.

        Args:
            project_id: Google Cloud Project ID, used to initialize vertexai
            location: Google Cloud Region, used to initialize vertexai
            image_path: Path to image (local or Google Cloud Storage) to generate embeddings for.
            video_path: Path to video (local or Google Cloud Storage) to generate embeddings for.
            contextual_text: Text to generate embeddings for.
            dimension: Dimension for the returned embeddings.
                https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#low-dimension
            video_segment_config: Define specific segments to generate embeddings for.
                https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#video-best-practices
        Returns:
            MultiModalEmbeddingResponse: A container object holding the embeddings for the provided image, video, and text inputs.
                The embeddings are dense vectors representing the semantic meaning of the inputs.
                Embeddings can be accessed as follows:
                - embeddings.image_embedding (numpy.ndarray): Embedding for the provided image.
                - embeddings.video_embeddings (List[VideoEmbedding]): List of embeddings for video segments.
                - embeddings.text_embedding (numpy.ndarray): Embedding for the provided text.
        """

        # æ£€æŸ¥å¹¶æˆªæ–­æ–‡æœ¬é•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡Google APIé™åˆ¶
        MAX_TEXT_LEN = 900  # è¿›ä¸€æ­¥å‡å°‘ï¼Œç¡®ä¿å®‰å…¨è¾¹ç•Œ
        if contextual_text:
            print(f"ğŸ” ä¼ å…¥Google APIçš„æ–‡æœ¬é•¿åº¦: {len(contextual_text)}")
            print(f"ğŸ” æ–‡æœ¬å‰100å­—ç¬¦: {contextual_text[:100]}...")
            print(f"ğŸ” æ–‡æœ¬å100å­—ç¬¦: {contextual_text[-100:] if len(contextual_text) > 100 else contextual_text}")
            
            if len(contextual_text) > MAX_TEXT_LEN:
                original_len = len(contextual_text)
                contextual_text = contextual_text[:MAX_TEXT_LEN]
                print(f"âš ï¸ æ–‡æœ¬é•¿åº¦è¶…é™ï¼Œå·²æˆªæ–­: {original_len} -> {MAX_TEXT_LEN} å­—ç¬¦")
                print(f"æˆªæ–­åçš„æ–‡æœ¬: {contextual_text[:100]}...")
            else:
                print(f"âœ… æ–‡æœ¬é•¿åº¦åœ¨å®‰å…¨èŒƒå›´å†…: {len(contextual_text)} å­—ç¬¦")

        # å°è¯•å¤šä¸ªåŒºåŸŸï¼Œå› ä¸ºmultimodal embeddingæœåŠ¡å¯èƒ½ä¸åœ¨æ‰€æœ‰åŒºåŸŸéƒ½å¯ç”¨
        regions_to_try = [
            self.location,  # é¦–å…ˆå°è¯•é…ç½®çš„åŒºåŸŸ
            "us-central1",  # ç„¶åå°è¯•us-central1
            "us-east1",     # å†å°è¯•us-east1
            "europe-west1"  # æœ€åå°è¯•europe-west1
        ]
        
        last_error = None
        
        for region in regions_to_try:
            try:
                print(f"ğŸ”„ å°è¯•åœ¨åŒºåŸŸ {region} è·å–multimodal embedding...")
                
                # é‡æ–°åˆå§‹åŒ–vertexai
                vertexai.init(project=self.project_id, location=region)
                
                # å°è¯•è·å–æ¨¡å‹
                model = MultiModalEmbeddingModel.from_pretrained("multimodalembedding")
                
                image, video = None, None
                if image_path is not None:
                    image = Image.load_from_file(image_path)
                if video_path is not None:
                    video = Video.load_from_file(video_path)

                embeddings = model.get_embeddings(
                    image=image,
                    video=video,
                    video_segment_config=video_segment_config,
                    contextual_text=contextual_text,
                    dimension=dimension,
                )
                
                print(f"âœ… æˆåŠŸåœ¨åŒºåŸŸ {region} è·å–embedding")
                return embeddings
                
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ åœ¨åŒºåŸŸ {region} å¤±è´¥: {error_msg}")
                last_error = e
                
                # å¦‚æœæ˜¯404é”™è¯¯ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªåŒºåŸŸ
                if "404" in error_msg or "Servable not found" in error_msg:
                    continue
                else:
                    # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼Œå¯èƒ½ä¸éœ€è¦ç»§ç»­å°è¯•
                    break
        
        # å¦‚æœæ‰€æœ‰åŒºåŸŸéƒ½å¤±è´¥äº†ï¼Œå°è¯•ä½¿ç”¨fallbackæ–¹æ³•
        if last_error:
            print(f"âš ï¸ Google Cloud multimodal embeddingæœåŠ¡åœ¨æ‰€æœ‰åŒºåŸŸéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨fallbackæ–‡æœ¬embedding")
            print(f"æœ€åä¸€ä¸ªé”™è¯¯: {str(last_error)}")
            
            # ä½¿ç”¨fallbackæ–‡æœ¬embedding
            if contextual_text:
                fallback_embedding = self._get_fallback_text_embedding(contextual_text)
                
                # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„MultiModalEmbeddingResponseå¯¹è±¡
                class FallbackEmbeddingResponse:
                    def __init__(self, text_embedding):
                        self.text_embedding = text_embedding
                        self.image_embedding = None
                        self.video_embeddings = None
                
                return FallbackEmbeddingResponse(fallback_embedding)
            else:
                raise Exception("æ²¡æœ‰æä¾›æ–‡æœ¬å†…å®¹ï¼Œæ— æ³•ä½¿ç”¨fallbackæ–¹æ³•")
        else:
            raise Exception("æ— æ³•è·å–multimodal embeddingï¼Œè¯·æ£€æŸ¥Google Cloudé…ç½®")

    def init_embedding(self, id):
        """
        åˆå§‹åŒ–åµŒå…¥ï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼Œä½†ä¸å†éœ€è¦ï¼‰
        
        Args:
            id: è§†é¢‘ID
        """
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨ä¸éœ€è¦åšä»»ä½•äº‹æƒ…ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯ç›´æ¥ä½¿ç”¨Google embedding
        pass

    def rerank(self, target_vid, video_embeddings, current_query_embedding):
        """
        ä½¿ç”¨å½“å‰æŸ¥è¯¢åµŒå…¥è¿›è¡Œé‡æ’åº
        
        Args:
            target_vid: ç›®æ ‡è§†é¢‘ID
            video_embeddings: è§†é¢‘åº“çš„åµŒå…¥çŸ©é˜µ
            current_query_embedding: å½“å‰æŸ¥è¯¢çš„embeddingï¼ˆå¯èƒ½æ˜¯Google embeddingæˆ–fallback embeddingï¼‰
            
        Returns:
            top_k_ids: é‡æ’åºåçš„top-kè§†é¢‘IDåˆ—è¡¨
            desired_video_rank: ç›®æ ‡è§†é¢‘çš„æ’å
        """
        # æ£€æŸ¥ç»´åº¦åŒ¹é…
        query_dim = len(current_query_embedding)
        
        # å¤„ç†video_embeddingså¯èƒ½æ˜¯listæˆ–numpy arrayçš„æƒ…å†µ
        if isinstance(video_embeddings, list):
            video_dim = len(video_embeddings[0]) if video_embeddings else 0
        else:
            video_dim = video_embeddings.shape[1] if len(video_embeddings.shape) > 1 else len(video_embeddings[0])
        
        if query_dim != video_dim:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…: æŸ¥è¯¢embeddingç»´åº¦={query_dim}, è§†é¢‘embeddingç»´åº¦={video_dim}")
            
            # å¦‚æœä½¿ç”¨fallback embedding (384ç»´) è€Œè§†é¢‘åº“æ˜¯Google embedding (1408ç»´)
            if query_dim == 384 and video_dim == 1408:
                print("ğŸ”„ æ£€æµ‹åˆ°fallback embeddingï¼Œéœ€è¦é‡æ–°ç”Ÿæˆè§†é¢‘åº“çš„fallback embedding")
                
                # è¿™é‡Œæˆ‘ä»¬éœ€è¦é‡æ–°ç”Ÿæˆè§†é¢‘åº“çš„fallback embedding
                # ä½†æ˜¯ç”±äºæˆ‘ä»¬æ²¡æœ‰è§†é¢‘çš„æ–‡æœ¬æè¿°ï¼Œæˆ‘ä»¬åªèƒ½ä½¿ç”¨åŸå§‹çš„æ–‡æœ¬embedding
                # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªåŸºäºåŸå§‹æ’åçš„ç»“æœ
                print("âš ï¸ æ— æ³•è¿›è¡Œç»´åº¦åŒ¹é…çš„é‡æ’åºï¼Œè¿”å›åŸå§‹æ’å")
                
                # è¿”å›ä¸€ä¸ªç®€å•çš„æ’åï¼ˆåŸºäºç´¢å¼•é¡ºåºï¼‰
                top_k_ids = []
                for i in range(min(10, len(self.memories))):
                    top_k_ids.append(self.memories[i]["video"].replace(self.video_ext, ""))
                
                # æ‰¾åˆ°ç›®æ ‡è§†é¢‘çš„æ’å
                desired_video_rank = None
                for idx, memory in enumerate(self.memories):
                    if memory["video"].replace(self.video_ext, "") == target_vid:
                        desired_video_rank = idx + 1
                        break
                
                return top_k_ids, desired_video_rank
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç»´åº¦ç»„åˆ: æŸ¥è¯¢={query_dim}, è§†é¢‘={video_dim}")
        
        # ç»´åº¦åŒ¹é…ï¼Œæ­£å¸¸è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—
        similarities = cosine_similarity([current_query_embedding], video_embeddings)

        # è¯„ä¼°top-kæ£€ç´¢å‡†ç¡®ç‡
        k_values = [10]
        top_k_ids = []
        desired_video_rank = None
        
        for k in k_values:
            top_k_indices = np.argsort(-similarities[0])
            
            # æ‰¾åˆ°ç›®æ ‡è§†é¢‘çš„æ’å
            for idx, k_index in enumerate(top_k_indices):
                if self.memories[k_index]["video"].replace(self.video_ext, "") == target_vid:
                    desired_video_rank = idx + 1
                    break

            # è·å–top-kè§†é¢‘ID
            top_k_indices = top_k_indices[:k]
            for idx in top_k_indices:
                top_k_ids.append(self.memories[idx]["video"].replace(self.video_ext, ""))
        
        return top_k_ids, desired_video_rank
    
    def reset_reformatter(self, initial_description: str = ""):
        """
        é‡ç½®é‡æ„å™¨çŠ¶æ€
        
        Args:
            initial_description: åˆå§‹æè¿°æ–‡æœ¬
        """
        self.reformatter.reset_reformatter(initial_description)
    
    def reformat_dialogue(self, question: str, answer: str, max_tokens: int = 500) -> str:
        """
        é‡æ„å¯¹è¯æ—¥å¿—ï¼Œç”Ÿæˆæ–°çš„æè¿°æ–‡æœ¬
        
        Args:
            question: å½“å‰è½®æ¬¡çš„é—®é¢˜
            answer: å½“å‰è½®æ¬¡çš„ç­”æ¡ˆ
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            é‡æ„åçš„æè¿°æ–‡æœ¬
        """
        return self.reformatter.reformat_dialogue(question, answer, max_tokens)
    
    def get_current_description(self) -> str:
        """
        è·å–å½“å‰æè¿°æ–‡æœ¬
        
        Returns:
            å½“å‰æè¿°æ–‡æœ¬
        """
        return self.reformatter.get_current_description()
    
    def get_reformatter_history(self) -> List[Dict[str, str]]:
        """
        è·å–é‡æ„å™¨å†å²è®°å½•
        
        Returns:
            é‡æ„å™¨å†å²è®°å½•åˆ—è¡¨
        """
        return self.reformatter.get_reformatter_history()
